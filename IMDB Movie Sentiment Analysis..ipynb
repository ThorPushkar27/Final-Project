{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ce2e520",
   "metadata": {},
   "source": [
    "# Step 1: Download the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19eb856",
   "metadata": {},
   "source": [
    "**I have already downloaded the \"Large Movie Review DataSet v1.0\". for you to download the dataset simply go to this link \"https://ai.stanford.edu/~amaas/data/sentiment/\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401ef65b",
   "metadata": {},
   "source": [
    "**Note: IMDB lets the user to rate movies on a scale from 1 to 10. The curator of the dataset labelled anything with <= 4 stars as negative and >=7 stars as positive. Reviews with 5 or 6 stars has been left out.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008daeef",
   "metadata": {},
   "source": [
    "# Step 2: Reading the dataset in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6810f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_train = []\n",
    "for line in open('full_train.txt','r', encoding=\"utf8\"):\n",
    "    review_train.append(line.strip())\n",
    "\n",
    "review_test = []\n",
    "for line in open('full_test.txt','r', encoding=\"utf8\"):\n",
    "    review_test.append(line.strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b9380a",
   "metadata": {},
   "source": [
    "# Step 3: Cleaning and Preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1efee19",
   "metadata": {},
   "source": [
    "**The raw text is pretty messy for these reviews so before we can do any analytics we need to clean things up.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfffe22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Removing all the punctuation marks and HTML tags.\n",
    "replace_no_space = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "replace_with_space = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "def preprocess_review(reviews):\n",
    "    reviews = [replace_no_space.sub(\"\",line.lower()) for line in reviews]\n",
    "    reviews = [replace_with_space.sub(\" \",line) for line in reviews]\n",
    "    return reviews\n",
    "\n",
    "review_train_clean = preprocess_review(review_train)\n",
    "review_test_clean = preprocess_review(review_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c179ca17",
   "metadata": {},
   "source": [
    "**This is how my review looks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3de1e30c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_train_clean[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc639b8f",
   "metadata": {},
   "source": [
    "**Vectorization**: In order for this data to make sense to our machine learning algorithm we’ll need to convert each review to a numeric representation, which we call vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eaf1498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding using CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(binary = True)\n",
    "cv.fit(review_train_clean)\n",
    "X = cv.transform(review_train_clean)\n",
    "X_test = cv.transform(review_test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aca9d4e",
   "metadata": {},
   "source": [
    "# Step 4: Building Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20896b3",
   "metadata": {},
   "source": [
    "**Logistic Regression is a good baseline model for this project because of these reasons:\n",
    "(1) They are easy to interpret\n",
    "(2) Linear model tends to perform well on sparse datasets like this one.\n",
    "(3) They learn very fast as compared to other model because of their simplicity.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b82d03",
   "metadata": {},
   "source": [
    "**Note**:The targets/labels we use will be same for the training as well as test dataset because both are structured in the same way, where the first 12.5k reviews are positive and the last 12.5k reviews are negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b26f08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.8752\n",
      "Accuracy for C=0.05: 0.88448\n",
      "Accuracy for C=0.25: 0.8824\n",
      "Accuracy for C=0.5: 0.87696\n",
      "Accuracy for C=1: 0.87616\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = [1 if i<12500 else 0 for i in range(25000)]\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, target, train_size = 0.75)\n",
    "\n",
    "for c in [0.01, 0.05,0.25, 0.5, 1]:\n",
    "    lr = LogisticRegression(C=c, solver = 'lbfgs', max_iter = 1000)\n",
    "    lr.fit(X_train, Y_train)\n",
    "    print(\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(Y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fcd7cc",
   "metadata": {},
   "source": [
    "**The value of c that gives the highest accuracy is 0.05.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df72c00",
   "metadata": {},
   "source": [
    "# Training the final model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1d89be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.88156\n"
     ]
    }
   ],
   "source": [
    "final_model = LogisticRegression(C=0.05, solver=\"lbfgs\", max_iter=1000)\n",
    "final_model.fit(X,target)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(target, final_model.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0401ad4",
   "metadata": {},
   "source": [
    "As a sanity check, let's look at the 5 most discriminating words both for positive and negative reviews.We will do this by looking at the largest and the smallest coefficients, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5943f6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('excellent', 0.9287783331979843)\n",
      "('perfect', 0.7916858965240479)\n",
      "('great', 0.6740498048937984)\n",
      "('amazing', 0.6131909605881833)\n",
      "('superb', 0.6010839559331261)\n"
     ]
    }
   ],
   "source": [
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        cv.get_feature_names(), final_model.coef_[0]\n",
    "    )\n",
    "}\n",
    "for best_positive in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1], \n",
    "    reverse=True)[:5]:\n",
    "    print (best_positive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1d42280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('worst', -1.3646347396142875)\n",
      "('waste', -1.1668254583062538)\n",
      "('awful', -1.0321153550729136)\n",
      "('poorly', -0.8751983530173937)\n",
      "('boring', -0.8567551153619931)\n"
     ]
    }
   ],
   "source": [
    "for best_negative in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1])[:5]:\n",
    "    print (best_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581795d4",
   "metadata": {},
   "source": [
    "# Step 5: More Text Preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d384e3c",
   "metadata": {},
   "source": [
    "**For our first iteration, we did a very basic stuff like removing puctution marks, HTML tags and making everything in the lower case. Now, we will further clean the text by removing stopwords and normalizing the text.**\n",
    "\n",
    "To do this transformation, we will use liberaries like Natural Language Toolkit(NLTK). This is a very popular library in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03f68ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Pushkar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "englist_stop_words = stopwords.words('english')\n",
    "def remove_stop_words(corpus):\n",
    "    removed_stop_words = [];\n",
    "    for review in corpus:\n",
    "        removed_stop_words.append(\n",
    "            ' '.join([word for word in review.split()\n",
    "                     if word not in englist_stop_words])\n",
    "        )\n",
    "    return removed_stop_words\n",
    "\n",
    "final_review = remove_stop_words(review_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18102692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      " \n",
      "bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt\n",
      " \n",
      "After\n",
      " \n",
      "bromwell high cartoon comedy ran time programs school life teachers 35 years teaching profession lead believe bromwell highs satire much closer reality teachers scramble survive financially insightful students see right pathetic teachers pomp pettiness whole situation remind schools knew students saw episode student repeatedly tried burn school immediately recalled high classic line inspector im sack one teachers student welcome bromwell high expect many adults age think bromwell high far fetched pity isnt\n"
     ]
    }
   ],
   "source": [
    "print(\"Before\")\n",
    "print(\" \")\n",
    "print(review_train_clean[0])\n",
    "print(\" \")\n",
    "print('After')\n",
    "print(\" \")\n",
    "print(final_review[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2178a925",
   "metadata": {},
   "source": [
    "**A common next step in text preprocessing is to normalize the words in your corpus by trying to convert all the different forms of the given words into one. There are two methods exists for this. 1. Stemming 2. Lemmatization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f727ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "def get_stemmed_text(corpus):\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    return [' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]\n",
    "\n",
    "stemmed_reviews = get_stemmed_text(review_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed379214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "def get_lemmatized_text(corpus):\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]\n",
    "\n",
    "lemmatized_reviews = get_lemmatized_text(review_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f454873e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Normalization\n",
      " \n",
      "bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled  at  high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt\n",
      " \n",
      "Stemming\n",
      " \n",
      "bromwel high is a cartoon comedi it ran at the same time as some other program about school life such as teacher my 35 year in the teach profess lead me to believ that bromwel high satir is much closer to realiti than is teacher the scrambl to surviv financi the insight student who can see right through their pathet teacher pomp the petti of the whole situat all remind me of the school i knew and their student when i saw the episod in which a student repeatedli tri to burn down the school i immedi recal at high a classic line inspector im here to sack one of your teacher student welcom to bromwel high i expect that mani adult of my age think that bromwel high is far fetch what a piti that it isnt\n",
      " \n",
      "Lemmatization\n",
      " \n",
      "bromwell high is a cartoon comedy it ran at the same time a some other program about school life such a teacher my 35 year in the teaching profession lead me to believe that bromwell high satire is much closer to reality than is teacher the scramble to survive financially the insightful student who can see right through their pathetic teacher pomp the pettiness of the whole situation all remind me of the school i knew and their student when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled at high a classic line inspector im here to sack one of your teacher student welcome to bromwell high i expect that many adult of my age think that bromwell high is far fetched what a pity that it isnt\n"
     ]
    }
   ],
   "source": [
    "print(\"No Normalization\")\n",
    "print(\" \")\n",
    "print(review_train_clean[0])\n",
    "print(\" \")\n",
    "print(\"Stemming\")\n",
    "print(\" \")\n",
    "print(stemmed_reviews[0])\n",
    "print(\" \")\n",
    "print(\"Lemmatization\")\n",
    "print(\" \")\n",
    "print(lemmatized_reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b6b919",
   "metadata": {},
   "source": [
    "**n-gram**:Previously, we used only single word features in our model, which we call 1-grams or unigrams. We can potentially add more predictive power to our model by adding two or three word sequences (bigrams or trigrams) as well. For example, if a review had the three word sequence “didn’t love movie” we would only consider these words individually with a unigram-only model and probably not capture that this is actually a negative sentiment because the word ‘love’ by itself is going to be highly correlated with a positive review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1860d00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.884\n",
      "Accuracy for C=0.05: 0.89008\n",
      "Accuracy for C=0.25: 0.8928\n",
      "Accuracy for C=0.5: 0.89344\n",
      "Accuracy for C=1: 0.89424\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
    "ngram_vectorizer.fit(review_train_clean)\n",
    "X = ngram_vectorizer.transform(review_train_clean)\n",
    "X_test = ngram_vectorizer.transform(review_test_clean)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X, target, train_size = 0.75\n",
    ")\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c, solver='lbfgs', max_iter=1000)\n",
    "    lr.fit(X_train, Y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(Y_val, lr.predict(X_val))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e236a83",
   "metadata": {},
   "source": [
    "**Clearly, c-1 is giving us the best accuracy close to 90%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03a01a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.89728\n"
     ]
    }
   ],
   "source": [
    "# final model.\n",
    "final_ngram = LogisticRegression(C=1, solver='lbfgs', max_iter=1000)\n",
    "final_ngram.fit(X, target)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(target, final_ngram.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c421e5f",
   "metadata": {},
   "source": [
    "**Getting accuracy pretty close to 90%! so simply considering 2-word seq instead of 1 word increased our accuracy by more than 1.6 percentage**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e876188",
   "metadata": {},
   "source": [
    "# Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982909bd",
   "metadata": {},
   "source": [
    "**Word Counts**: Instead of simply noting whether a word appears in the review or not, we can include the number of times a given word appears. This can give our sentiment classifier a lot more predictive power. For example, if a movie reviewer says ‘amazing’ or ‘terrible’ multiple times in a review it is considerably more probable that the review is positive or negative, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "15b036b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.88112\n",
      "Accuracy for C=0.05: 0.88576\n",
      "Accuracy for C=0.25: 0.88208\n",
      "Accuracy for C=0.5: 0.87984\n",
      "Accuracy for C=1: 0.87888\n",
      "Final Accuracy: 0.8822\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wc_vectorizer = CountVectorizer(binary=False)\n",
    "wc_vectorizer.fit(review_train_clean)\n",
    "X = wc_vectorizer.transform(review_train_clean)\n",
    "X_test = wc_vectorizer.transform(review_test_clean)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, target, train_size = 0.75, \n",
    ")\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c, solver='lbfgs', max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
    "\n",
    "    \n",
    "final_wc = LogisticRegression(C=0.05, solver='lbfgs', max_iter=1000)\n",
    "final_wc.fit(X, target)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(target, final_wc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872d7968",
   "metadata": {},
   "source": [
    "**TF-IDF**:Another common way to represent each document in a corpus is to use the tf-idf statistic (term frequency-inverse document frequency) for each word, which is a weighting factor that we can use in place of binary or word count representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6a2b2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.792\n",
      "Accuracy for C=0.05: 0.8256\n",
      "Accuracy for C=0.25: 0.86384\n",
      "Accuracy for C=0.5: 0.87728\n",
      "Accuracy for C=1: 0.88608\n",
      "Final Accuracy: 0.8676\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(review_train_clean)\n",
    "X = tfidf_vectorizer.transform(review_train_clean)\n",
    "X_test = tfidf_vectorizer.transform(review_test_clean)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, target, train_size = 0.75\n",
    ")\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    lr = LogisticRegression(C=c, solver='lbfgs', max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, lr.predict(X_val))))\n",
    "    \n",
    "final_tfidf = LogisticRegression(C=0.25, solver='lbfgs', max_iter=1000)\n",
    "final_tfidf.fit(X, target)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(target, final_tfidf.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dd001c",
   "metadata": {},
   "source": [
    "So far we’ve chosen to represent each review as a very sparse vector (lots of zeros!) with a slot for every unique n-gram in the corpus (minus n-grams that appear too often or not often enough). Linear classifiers typically perform better than other algorithms on data that is represented in this way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ef5b59",
   "metadata": {},
   "source": [
    "**Support Vector Machines(SVM)**:\n",
    "Since Linear Classifiers tends to work well on very sparse dataset(like the one we have). Another algorithm that can produce great results with a quick training time are Support Vector Machines with a linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a128b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.89056\n",
      "Accuracy for C=0.05: 0.88784\n",
      "Accuracy for C=0.25: 0.8864\n",
      "Accuracy for C=0.5: 0.88624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pushkar\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=1: 0.88608\n",
      "Final Accuracy: 0.89708\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
    "ngram_vectorizer.fit(review_train_clean)\n",
    "X = ngram_vectorizer.transform(review_train_clean)\n",
    "X_test = ngram_vectorizer.transform(review_test_clean)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, target, train_size = 0.75\n",
    ")\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:\n",
    "    \n",
    "    svm = LinearSVC(C=c, max_iter=1500)\n",
    "    svm.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, svm.predict(X_val))))\n",
    "\n",
    "    \n",
    "final_svm_ngram = LinearSVC(C=0.01, max_iter=1500)\n",
    "final_svm_ngram.fit(X, target)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(target, final_svm_ngram.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cb5493",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a6b7e1",
   "metadata": {},
   "source": [
    "**I have found that removing a small set of stopwords along with a n-gram range from 1 to 3 and a linear SVC gave me the best result.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f1fd9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.001: 0.8936\n",
      "Accuracy for C=0.005: 0.89712\n",
      "Accuracy for C=0.01: 0.89776\n",
      "Accuracy for C=0.05: 0.89872\n",
      "Accuracy for C=0.1: 0.89872\n",
      "Final Accuracy: 0.90024\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "stop_words = ['in', 'of', 'at', 'a', 'the']\n",
    "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 3), stop_words=stop_words)\n",
    "ngram_vectorizer.fit(review_train_clean)\n",
    "X = ngram_vectorizer.transform(review_train_clean)\n",
    "X_test = ngram_vectorizer.transform(review_test_clean)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, target, train_size = 0.75\n",
    ")\n",
    "\n",
    "for c in [0.001, 0.005, 0.01, 0.05, 0.1]:\n",
    "    \n",
    "    svm = LinearSVC(C=c, max_iter=1000)\n",
    "    svm.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_val, svm.predict(X_val))))\n",
    "\n",
    "    \n",
    "final = LinearSVC(C=0.01, max_iter=1000)\n",
    "final.fit(X, target)\n",
    "print (\"Final Accuracy: %s\" \n",
    "       % accuracy_score(target, final.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3887e997",
   "metadata": {},
   "source": [
    "**Hurray!, I have achived 90% mark!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d73e4b",
   "metadata": {},
   "source": [
    "**Summary**: We have gone over several options for transforming text that can improve the accuracy of an NLP model. Which combination of these techniques will yield the best results will depend on the task, data representations, and the algorithm you choose. It's always a good idea to try out different combinations to see what works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bd096a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
